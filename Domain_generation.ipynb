{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c8dd94-7c19-43b9-930c-30856a8aedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "from random import randint\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4e4fa5-0d9a-4741-9583-54e3c4678652",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_conf = (\n",
    "SparkConf()\n",
    ".set(\"spark.jars.packages\", 'org.apache.hadoop:hadoop-client:3.3.0,org.apache.hadoop:hadoop-aws:3.3.0')\n",
    ".set(\"spark.driver.memory\", \"6g\")\n",
    ".set(\"spark.hadoop.fs.s3a.endpoint\", \"minio:9000\")\n",
    ".set(\"spark.hadoop.fs.s3a.access.key\", \"Hd8vo0i3RJIq7vmIwz6T\")\n",
    ".set(\"spark.hadoop.fs.s3a.secret.key\",\"gUv5gRpOKvZOxnw6tcApgXcmwvmoZwspA8wGoj8m\" )\n",
    ".set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    ".set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    ".set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    ".set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    ")\n",
    "sc = SparkContext.getOrCreate(spark_conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357000d8-8627-4d82-bbc3-63fb80e72073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version = 3.3.2\n"
     ]
    }
   ],
   "source": [
    " print(f\"Hadoop version = {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c15fd7-ba27-461a-8e9b-c241e9879d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "M=12500\n",
    "K=150\n",
    "\n",
    "documentCurrency = [\"EUR\", \"USD\", \"JPY\", \"GBR\", \"CHF\"]\n",
    "documentService = [\"Tax\", \"Rent\", \"Loan\", \"Insurance\", \"Utility\"]\n",
    "\n",
    "from random import randint\n",
    "from datetime import datetime,timedelta\n",
    "def invoiceGenerator(k=5):\n",
    "  postingDate = datetime(2022,1,1)+timedelta(randint(0,200))\n",
    "  return {\n",
    "          \"customerId\":\"Customer_{customerId}\".format(customerId=str(randint(0,k)+1).zfill(3)),\n",
    "          \"amount\":randint(50,10000),\n",
    "          \"documentCurrency\":random.choice(documentCurrency), \n",
    "          \"postingDate\":postingDate.strftime(\"%Y-%m-%d\"),\n",
    "          \"dueDate\":(postingDate+timedelta(randint(0,60))).strftime(\"%Y-%m-%d\"),\n",
    "          \"fiscalYear\":postingDate.strftime(\"%Y\"),\n",
    "          \"documentService\":random.choice(documentService)\n",
    "         }\n",
    "\n",
    "def invoiceList(k=5,n=1000):\n",
    "  rawInvoiceList = [invoiceGenerator(k) for k in range(n)]\n",
    "  rawInvoiceList.sort(key=lambda row: row.get(\"postingDate\"))\n",
    "  for pos,val in enumerate(rawInvoiceList):\n",
    "    val[\"documentNumber\"]=\"2022-{docNum}\".format(docNum=str(pos).zfill(5))\n",
    "  return rawInvoiceList\n",
    "  \n",
    "myInvoiceList = invoiceList(K,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dde8a0-9af3-4223-aff8-ed0302388dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paymentGenerator(paymentList):\n",
    "  documentNumber = \"2022-{docNum}\".format(docNum=str(randint(0,len(paymentList)-1)).zfill(5))\n",
    "  payment = [k for k in paymentList if k.get(\"documentNumber\")==documentNumber][0]\n",
    "  postingDate = datetime.strptime(payment.get(\"postingDate\"),\"%Y-%m-%d\")\n",
    "  return { \n",
    "          \"documentNumber\":documentNumber,\n",
    "          \"paymentDate\":(postingDate+timedelta(randint(15,90))).strftime(\"%Y-%m-%d\"),\n",
    "          \"valuePaid\":randint(1,payment.get(\"amount\")),\n",
    "          \"documentCurrency\":payment.get(\"documentCurrency\")\n",
    "         }\n",
    "\n",
    "\n",
    "def paymentList(paymentList,m=250):\n",
    "  return [paymentGenerator(paymentList) for k in range(m)]\n",
    "   \n",
    "myPaymentList = paymentList(myInvoiceList,M)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72ed5de-35c0-4059-aac6-35cb1f919893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoice_rdd = sc.parallelize(myInvoiceList)\n",
    "invoice_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ed31c5-0047-40bd-a608-1fae4be37e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payment_rdd = sc.parallelize(myPaymentList)\n",
    "payment_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89222131-f286-45c4-8c21-1e0b55eae32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: long (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- documentCurrency: string (nullable = true)\n",
      " |-- documentNumber: string (nullable = true)\n",
      " |-- documentService: string (nullable = true)\n",
      " |-- dueDate: string (nullable = true)\n",
      " |-- fiscalYear: string (nullable = true)\n",
      " |-- postingDate: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['value', 'customerId', 'documentCurrency', 'documentNumber', 'documentService', 'dueDate', 'fiscalYear', 'postingDate']\n",
    "invoice_df = invoice_rdd.toDF(columns)\n",
    "invoice_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5389260-fd67-435b-a6c9-f1cd3c419f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value=3731, customerId='Customer_368', documentCurrency='EUR', documentNumber='2022-00000', documentService='Loan', dueDate='2022-02-02', fiscalYear='2022', postingDate='2022-01-01')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoice_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f4ee9f8-d6de-4ad4-bfca-ccc110c3d13d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- documentCurrency: string (nullable = true)\n",
      " |-- documentNumber: string (nullable = true)\n",
      " |-- paymentDate: string (nullable = true)\n",
      " |-- valuePaid: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payment_df = payment_rdd.toDF([\"documentCurrency\",\"documentNumber\",\"paymentDate\", \"valuePaid\"])\n",
    "payment_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb7d82-2aa1-4b8e-b733-808209d70737",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o105.save.\n: java.lang.NoSuchMethodError: 'void com.google.common.base.Preconditions.checkArgument(boolean, java.lang.String, java.lang.Object, java.lang.Object)'\n\tat org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:893)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:869)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.getEncryptionAlgorithm(S3AUtils.java:1580)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:558)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minvoice_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms3a://spark-demo/users\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o105.save.\n: java.lang.NoSuchMethodError: 'void com.google.common.base.Preconditions.checkArgument(boolean, java.lang.String, java.lang.Object, java.lang.Object)'\n\tat org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:893)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:869)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.getEncryptionAlgorithm(S3AUtils.java:1580)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:558)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "absolute_path = \"/tmp/my_parquet_data\"\n",
    "invoice_df.write.format('parquet').mode('overwrite').save('')\n",
    "\n",
    "invoice_df.write.format('parquet').mode('overwrite').save('s3a://spark-demo/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e283f5cd-e555-4422-8691-b123feeb461f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(documentCurrency='CHF', documentNumber='2022-06012', paymentDate='2022-05-19', valuePaid=2169)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payment_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "396110ee-6a99-4b3c-9d56-c56dab9fa6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------------+--------------+---------------+----------+----------+-----------+---------+\n",
      "|value|   customerId|documentCurrency|documentNumber|documentService|   dueDate|fiscalYear|postingDate|totalPaid|\n",
      "+-----+-------------+----------------+--------------+---------------+----------+----------+-----------+---------+\n",
      "| 6048| Customer_197|             USD|    2022-00264|      Insurance|2022-01-24|      2022| 2022-01-06|     1996|\n",
      "|  517|Customer_6420|             EUR|    2022-00534|        Utility|2022-01-19|      2022| 2022-01-11|      459|\n",
      "| 9796|Customer_7227|             GBR|    2022-00609|      Insurance|2022-02-09|      2022| 2022-01-12|     NULL|\n",
      "| 3203|Customer_8294|             CHF|    2022-00651|      Insurance|2022-02-07|      2022| 2022-01-13|     2126|\n",
      "| 5353| Customer_673|             EUR|    2022-00826|      Insurance|2022-03-05|      2022| 2022-01-17|      151|\n",
      "| 8309| Customer_575|             USD|    2022-00892|           Rent|2022-02-04|      2022| 2022-01-18|     2979|\n",
      "| 7064| Customer_174|             EUR|    2022-00399|           Loan|2022-03-09|      2022| 2022-01-09|     4842|\n",
      "| 5288|Customer_3453|             USD|    2022-00633|           Rent|2022-01-31|      2022| 2022-01-13|     NULL|\n",
      "| 2974|Customer_3331|             USD|    2022-00740|           Rent|2022-01-23|      2022| 2022-01-15|      686|\n",
      "| 9269|Customer_3974|             JPY|    2022-00804|      Insurance|2022-01-21|      2022| 2022-01-16|     6921|\n",
      "| 9917|Customer_6693|             CHF|    2022-00856|      Insurance|2022-02-27|      2022| 2022-01-17|     9159|\n",
      "| 9568|Customer_5049|             GBR|    2022-00284|           Loan|2022-01-29|      2022| 2022-01-06|     NULL|\n",
      "| 2953|Customer_4752|             JPY|    2022-00966|           Rent|2022-01-26|      2022| 2022-01-19|     NULL|\n",
      "| 3941| Customer_202|             GBR|    2022-00163|           Rent|2022-02-08|      2022| 2022-01-04|     3164|\n",
      "| 6166| Customer_210|             JPY|    2022-00309|           Rent|2022-02-04|      2022| 2022-01-06|     4473|\n",
      "| 3860|Customer_3241|             USD|    2022-00415|           Loan|2022-01-11|      2022| 2022-01-09|     1732|\n",
      "| 2100|Customer_3679|             EUR|    2022-00627|            Tax|2022-01-26|      2022| 2022-01-13|     2093|\n",
      "| 2761|Customer_5775|             CHF|    2022-00692|           Rent|2022-03-14|      2022| 2022-01-14|      756|\n",
      "| 8812|Customer_2790|             JPY|    2022-00697|            Tax|2022-02-15|      2022| 2022-01-14|     NULL|\n",
      "|  392|Customer_1542|             JPY|    2022-00824|      Insurance|2022-03-08|      2022| 2022-01-17|      333|\n",
      "+-----+-------------+----------------+--------------+---------------+----------+----------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# REPORT WHICH SHOWS THE UNPAID INVOICES\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "payment_sum_df = payment_df.groupBy(\"documentNumber\").agg(F.sum(\"valuePaid\").alias(\"totalPaid\"))\n",
    "\n",
    "joined_df = invoice_df.join(payment_sum_df, on=\"documentNumber\", how=\"left\")\n",
    "\n",
    "unpaid_invoices_df = joined_df.filter(joined_df[\"value\"] > F.coalesce(joined_df[\"totalPaid\"], F.lit(0)))\n",
    "\n",
    "result_df = unpaid_invoices_df.select(\"value\", \"customerId\", \"documentCurrency\", \"documentNumber\", \"documentService\", \"dueDate\", \"fiscalYear\", \"postingDate\", \"totalPaid\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e5655-b3a8-4dac-9cd8-b1c69cc7eb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
